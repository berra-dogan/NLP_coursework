{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNMPVGE1tJRG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    RobertaTokenizerFast,\n",
        "    RobertaForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1K58n9-BvhAe"
      },
      "source": [
        "# Split Train and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_TEXT_MIN_LEN = 3\n",
        "\n",
        "def clean_df(df: pd.DataFrame):\n",
        "    sentence_len = df[\"text\"].str.len()\n",
        "    Q1 = sentence_len.quantile(0.25)\n",
        "    Q3 = sentence_len.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    train_text_max_len = 3 + 1.5 * IQR\n",
        "    outliers = df[(sentence_len < TRAIN_TEXT_MIN_LEN) | (sentence_len>train_text_max_len)]\n",
        "    outlier_percentage = len(outliers)/len(df) * 100\n",
        "    if outlier_percentage <= 5:\n",
        "        df = df.drop(outliers.index)\n",
        "    \n",
        "    return df\n",
        "    \n",
        "def x_y_split(df: pd.DataFrame):\n",
        "    y_categorical = df[\"PCL_category\"]\n",
        "    y_binary = df[\"is_PCL\"]\n",
        "    X = df.drop(columns=[\"PCL_category\", \"is_PCL\"])\n",
        "    return X, y_binary, y_categorical\n",
        "\n",
        "\n",
        "def data_read_split(data_path: str):\n",
        "    df = pd.read_csv(\n",
        "        data_path,\n",
        "        sep=\"\\t\",\n",
        "        skiprows=9,\n",
        "        engine=\"python\",\n",
        "        index_col=0,\n",
        "        header=None,\n",
        "        names = [\"article_id\", \"keyword\", \"country\", \"text\", \"PCL_category\"]\n",
        "    )\n",
        "    df[\"is_PCL\"] = df[\"PCL_category\"] >= 2\n",
        "    \n",
        "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "    # df_train = clean_df(df_train)\n",
        "    \n",
        "    return df_train, df_test    \n",
        "\n",
        "def data_preprocess(data_path: str): \n",
        "    df_train, df_test = data_read_split(data_path)\n",
        "    X_train, y_train_b, y_train_c = x_y_split(df_train)\n",
        "    X_test, y_test_b, y_test_c = x_y_split(df_test)\n",
        "    return X_train, X_test, y_train_b, y_test_b, y_train_c, y_test_c\n",
        "\n",
        "def t5_filtering(df: pd.DataFrame):\n",
        "    return df[[\"text\", \"is_PCL\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = \"dontpatronizeme_pcl.tsv\"\n",
        "df_train, df_test = data_read_split(data_path)\n",
        "df_train = t5_filtering(df_train)\n",
        "df_test = t5_filtering(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_pandas(df_train, preserve_index=False)\n",
        "test_dataset = Dataset.from_pandas(df_test, preserve_index=False)\n",
        "\n",
        "model_name = \"roberta-base\"\n",
        "num_labels = 2\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "    )\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset = train_dataset.rename_column(\"is_PCL\", \"labels\")\n",
        "test_dataset = test_dataset.rename_column(\"is_PCL\", \"labels\")\n",
        "\n",
        "train_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
        ")\n",
        "\n",
        "test_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
        ")\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds),\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
